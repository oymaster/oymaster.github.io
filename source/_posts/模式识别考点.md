---
title: 模式识别考点
categories:
  - 湘潭大学计算机研究生课程
  - 模式识别-陈殊
tags:
  - 湘潭大学
  - 考试资料
abbrlink: 33183
poster:
  topic: 标题上方的小字
  headline: 大标题
  caption: 标题下方的小字
  color: 标题颜色
date: 2025-1-4 14:21:54
updated: 2025-1-4  14:21:54
description:
cover:
banner:
sticky:
mermaid:
katex:
mathjax: true
topic:
author:
references:
comments:
indexing:
breadcrumb:
leftbar:
rightbar:
h1:
type:
---
## 一、简答题

### 1.简述kmeans基本步骤，参数k如何选择。 ###

1. 从数据中**选择 k 个对象作为初始聚类中心**；
2. 将样本集按照**最小距离原则**分配到最邻近聚类
3. 使用每个聚类的**样本均值**更新聚类中心；
4. 重复步骤（2）、（3），直到**聚类中心不再发生变化**；
5. 输出最终的聚类中心和 k 个簇划分；



k值可以根据领域知识来选择。例如，如果知道数据集中有多少个类别，就可以选择相应的k值。肘部法则和轮廓系数法也是常用的方法。



### 2.什么是模式识别，模式识别有什么意义。  ###

定义：是通过计算机用数学技术方法来研究模式的自动处理和判读。是对表征事物或现象的各种形式的(数值的、文字的和逻辑关系的)信息进行处理和分析，以对事物或现象进行描述、辨认、分类和解释的过程，是信息科学和人工智能的重要组成部分。

意义：人们为了掌握客观事物，按事物相似的程度组成类别。模式识别的作用和目的就在于面对某一具体事物时将其正确地归入某一类别。



### 3.模式识别预处理有哪些基本内容  ###

预处理是以去除噪声、加强有用的信息为目的, 并对输入测量仪器或其他因素所造成的退化现象进行复原的过程。

预处理一般有两种情况:

一是使数据的质量更好，比如用一些数字信号处理的方法去除信号中的噪声, 或者对一幅模糊的图像进行图像增强等, 确保有利于后期的模式识别工作；

另一种预处理是样本集的预处理, 比如样本集中异常值的剔除、类别的合并或分裂等。这一工作一般可以根据领域的专门知识进行, 也可以采用模式识别中的一些技术, 比如在进行后续工作之前先对样本集进行一次聚类分析。



### 4.最大似然方法和贝叶斯估计方法的差异

第一，最大似然方法预测时使用θ的点估计，贝叶斯方法使用θ的全分布。即最大似然求出最可能的θ值，而贝叶斯则是求解θ的分布。

第二，贝叶斯推断还引入了先验，通过先验和似然来求解后验分布，而最大似然直接使用似然函数，通过最大化其来求解。



### 5.简述svm的基本思想

支持向量机（support vector machines）是一种二分类模型，它的目的是寻找一个超平面来对样本进行分类，分类的原理是间隔最大化，最终转化为一个凸二次规划问题来求解，由简至繁的模型包括：

当训练样本线性可分时，通过硬间隔最大化，学习一个线性可分支持向量机；

当训练样本近似线性可分时，通过软间隔最大化，学习一个线性可分的支持向量机；

当训练样本线性不可分时，通过核函数和软间隔最大化，学习一个非线性支持向量机；



### 6.神经网络训练时是否可以将所有的参数初始化为0，为什么

**不可以**

1. **对称性问题：** 如果所有权重都初始化为相同的值，每个神经元学到的梯度将是相同的，导致所有神经元对应的权重在训练中都更新为相同的值，进而失去网络的表达能力。
2. **梯度更新问题：** 初始化所有权重为相同的值会导致反向传播中每个权重都接收相同的梯度，这样在权重更新时会使得所有权重按相同的步长进行更新，而不论它们在模型中的位置。



###  7.sigmod作为激烈(jilie)函数有什么缺点。

- **梯度消失：** Sigmoid函数在其输入非常大或非常小的情况下，导数趋于零，导致梯度消失问题。在反向传播时，梯度趋近于零，使得权重更新变得非常缓慢，导致训练过程变得很慢。
- **输出不以零为中心：** Sigmoid的输出范围是(0, 1)，因此其输出不以零为中心。这可能导致一些神经元在训练过程中出现偏置，从而使得某些神经元始终保持非常小的梯度。
- **幂运算相对耗时**。



### 8.简述k近邻分类方法

k近邻（k-Nearest Neighbors，简称k-NN）是一种基本的监督学习算法，主要用于分类问题。其基本思想是通过测量不同样本之间的距离，将一个新样本分配给距离最近的k个已知类别样本中最常见的类别。k-NN方法的简述如下：

1. 存储所有已知类别的样本。
2. 对于一个新样本，计算其与所有已知类别的样本的距离。
3. 从所有已知类别的样本中选择距离最近的k个样本。
4. 统计这k个样本中各个类别的出现次数，将出现次数最多的类别作为新样本的预测类别。



### 9.2006年以后使得深度神经网络得到快速应用的技术突破在哪里，举四个例子

1. **GPU计算的广泛应用：** 通用图形处理单元（GPU）在深度学习训练中的应用使得计算速度显著提升，加速了深度神经网络的训练过程。
2. **自监督学习和强化学习的兴起：** 自监督学习和强化学习等新兴学习范式为深度学习提供了更多的学习方式，扩展了深度神经网络在不同领域的应用。
3. **开源深度学习框架：** 出现了多个开源深度学习框架，如TensorFlow和PyTorch，简化了深度学习模型的实现和训练过程，促进了更广泛的研究和应用。



### 10.隐马尔可夫模型三个核心问题

1. **评估问题：**给定一个HHM的模型和观测序列，如何高效的计算此模型产生的观测序列的概率。
2. **解码问题：**给定一个HHM的模型和观测序列，如何选择对应最佳的序列，使它在某状态下最优（出现的概率最大），比较好地解释观测值。
3. **训练问题**：给定观测序列，如何调整模型参数，以使得观察序列出现的概率最大化。











































